{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Code for Testing Saved Model\n",
    "This file provides a sample to test the saved model. Make necessary changes so that we can test your CNN/RNN model with this file. If you developed a RNN model, change the name of this file to *test_rnn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data\n",
    "In the sample below, it loads the dev set for testing. But in real marking, the markers will load held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sent1</th>\n",
       "      <th>Sent2</th>\n",
       "      <th>SimScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A man with a hard hat is dancing.</td>\n",
       "      <td>A man wearing a hard hat is dancing.</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A young child is riding a horse.</td>\n",
       "      <td>A child is riding a horse.</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A man is feeding a mouse to a snake.</td>\n",
       "      <td>The man is feeding a mouse to the snake.</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A woman is playing the guitar.</td>\n",
       "      <td>A man is playing guitar.</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A woman is playing the flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>2995</td>\n",
       "      <td>The professor introduced the artists , and the...</td>\n",
       "      <td>The professor introduced the artists .</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>2996</td>\n",
       "      <td>The doctors supported the judges .</td>\n",
       "      <td>The doctors supported the tourists and the jud...</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>2997</td>\n",
       "      <td>The secretary knew the manager .</td>\n",
       "      <td>The secretary knew the manager danced .</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>2998</td>\n",
       "      <td>The professors next to the president recommend...</td>\n",
       "      <td>The president recommended the professors .</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>2999</td>\n",
       "      <td>The authors believed that the managers danced .</td>\n",
       "      <td>The managers danced .</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              Sent1  \\\n",
       "0              0                  A man with a hard hat is dancing.   \n",
       "1              1                   A young child is riding a horse.   \n",
       "2              2               A man is feeding a mouse to a snake.   \n",
       "3              3                     A woman is playing the guitar.   \n",
       "4              4                      A woman is playing the flute.   \n",
       "...          ...                                                ...   \n",
       "2995        2995  The professor introduced the artists , and the...   \n",
       "2996        2996                 The doctors supported the judges .   \n",
       "2997        2997                   The secretary knew the manager .   \n",
       "2998        2998  The professors next to the president recommend...   \n",
       "2999        2999    The authors believed that the managers danced .   \n",
       "\n",
       "                                                  Sent2  SimScore  \n",
       "0                  A man wearing a hard hat is dancing.      1.00  \n",
       "1                            A child is riding a horse.      0.95  \n",
       "2              The man is feeding a mouse to the snake.      1.00  \n",
       "3                              A man is playing guitar.      0.48  \n",
       "4                             A man is playing a flute.      0.55  \n",
       "...                                                 ...       ...  \n",
       "2995             The professor introduced the artists .      0.62  \n",
       "2996  The doctors supported the tourists and the jud...      0.68  \n",
       "2997            The secretary knew the manager danced .      0.37  \n",
       "2998         The president recommended the professors .      0.29  \n",
       "2999                              The managers danced .      0.43  \n",
       "\n",
       "[3000 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "\n",
    "dev_data = pd.read_csv('cw2_dev.csv')\n",
    "dev_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embeddings\n",
    "Clearly specify the embeddings your implementation requires. Also provide the link for downloading the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "<ipython-input-2-b8ff08c2ebb7>:16: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained glove embeddings\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "\n",
    "embd_name = 'glove.6B.300d'\n",
    "link_to_embd = 'https://nlp.stanford.edu/projects/glove/' # TODO: you should provide the link to download the embedding here\n",
    "\n",
    "# Below is a sample to load the glove embeddings. ADJUST the code according to the\n",
    "# embedding you want to use. \n",
    "word_vec_dim = 300\n",
    "path_of_downloaded_files = \"/Users/user/Desktop/NLP/glove.6B.{}d.txt\".format(word_vec_dim)\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Functions Needed for Evaluation\n",
    "All functions used to run and evaluate your model should be provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_sent_word_vecs(word_vectors, sent_words, largest_len):\n",
    "    vecs = []\n",
    "    for ww in sent_words:\n",
    "        if ww in word_vectors:\n",
    "            vecs.append(word_vectors[ww])\n",
    "        else:\n",
    "            vecs.append(oov_vec)\n",
    "    for i in range(largest_len-len(sent_words)):\n",
    "        vecs.append([0.]*word_vec_dim)\n",
    "    return np.array(np.transpose(vecs))\n",
    "\n",
    "def build_mini_batch(sent_list, word_vectors):\n",
    "    tokenized_sents = [word_tokenize(ss.lower()) for ss in sent_list]\n",
    "    largest_len = np.max([len(tokens) for tokens in tokenized_sents])\n",
    "    text_vecs = []\n",
    "    for ts in tokenized_sents:\n",
    "        vv = get_sent_word_vecs(word_vectors, ts, largest_len)\n",
    "        text_vecs.append(vv)\n",
    "    # print('mini batch shape',np.array(text_vecs).shape)\n",
    "    return np.array(text_vecs)\n",
    "\n",
    "def evaluate_trained_model(trained_model, dev_data):\n",
    "    dev_docs1 = dev_data['Sent1']\n",
    "    dev_docs2 = dev_data['Sent2']\n",
    "    dev_labels = dev_data['SimScore']\n",
    "    \n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval()\n",
    "        dev_predictions = []\n",
    "        for idx in range(0,len(dev_data),batch_size):\n",
    "            x_data1 = build_mini_batch(dev_docs1[idx:idx+batch_size], word_vectors)\n",
    "            x_data2 = build_mini_batch(dev_docs2[idx:idx+batch_size], word_vectors)\n",
    "            if x_data1.shape[0] == 0: continue # to avoid empty batch\n",
    "            elif x_data2.shape[0] == 0: continue\n",
    "            x_tensor1 = torch.tensor(x_data1, dtype=torch.float)\n",
    "            x_tensor2 = torch.tensor(x_data2, dtype=torch.float)\n",
    "\n",
    "            y_pred1 = model(x_tensor1).cpu().detach()#.numpy()\n",
    "            y_pred2 = model(x_tensor2).cpu().detach()#.numpy()\n",
    "\n",
    "            cos_sim = nn.CosineSimilarity()\n",
    "            pred_labels = cos_sim(y_pred1, y_pred2)\n",
    "            pred_labels = pred_labels.squeeze().tolist()\n",
    "\n",
    "            dev_predictions += pred_labels\n",
    "    \n",
    "    assert len(dev_labels) == len(dev_predictions)    \n",
    "    squared_errors = [np.square(ts-ps) for (ts, ps) in zip(dev_labels, dev_predictions)]\n",
    "    print('MSE of the method on the dev set:', np.mean(squared_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Your Model\n",
    "You should provide the implementaiton of your encoder model below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, embd_dim, filter_size_list, filter_num_list, out_dim, dp_rate=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        assert len(filter_size_list) == len(filter_num_list)\n",
    "        self.output_dim = out_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.fc = nn.Linear(np.sum(filter_num_list), out_dim)\n",
    "        self.convs = self.build_convs(filter_size_list, filter_num_list)\n",
    "        \n",
    "    def build_convs(self, f_sizes, f_nums):\n",
    "        convs = nn.ModuleList()\n",
    "        for fs, fn in zip(f_sizes, f_nums):\n",
    "            padding_size = fs-1\n",
    "            m = nn.Conv1d(self.embd_dim, fn, fs, padding=padding_size)\n",
    "            convs.append(m)\n",
    "        return convs\n",
    "    \n",
    "    def get_conv_output(self, input_matrix, conv):\n",
    "        # step 1: compute convolution\n",
    "        assert input_matrix.shape[1] == self.embd_dim\n",
    "        conv_output = conv(input_matrix)\n",
    "        # step 2: pass through an activation function\n",
    "        conv_relu = self.relu(conv_output)\n",
    "        # step 3: max-over-time pooling\n",
    "        maxp = nn.MaxPool1d(conv_relu.shape[2])\n",
    "        maxp_output = maxp(conv_relu)\n",
    "        return maxp_output\n",
    "       \n",
    "    def forward(self, all_text_vectors):\n",
    "        cnn_repr = torch.tensor([])\n",
    "        for cv in self.convs:\n",
    "            cv_output = self.get_conv_output(all_text_vectors, cv)\n",
    "            cnn_repr = torch.cat((cnn_repr, cv_output), dim=1)\n",
    "        # print(cnn_repr.shape)\n",
    "        after_dp = self.dropout(cnn_repr.squeeze())\n",
    "        logit = self.fc(after_dp)\n",
    "        # the CrossEntropyLoss provided by pytorch includes softmax; so you do not need to include a softmax layer in your net\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Evaluate Model\n",
    "The code below creates an instance of the model, loads the saved weights (sample_model.state_dict; run cw2_sample.ipynb will generate this file), and tests it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the method on the dev set: 0.03424460890371057\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# load the saved file\n",
    "with open('best_cnn.state_dict','rb') as ff:\n",
    "    saved_info = pickle.load(ff)\n",
    "    \n",
    "# extract the information from the saved file\n",
    "oov_vec = saved_info['oov_vec']\n",
    "saved_model_state = saved_info['model_state_dict']\n",
    "\n",
    "# create model, load saved weights, and test the model\n",
    "filter_sizes = [2,3,4]\n",
    "filter_nums = [100]*len(filter_sizes)\n",
    "dropout_rate = 0\n",
    "batch_size = 50\n",
    "model = CNN(word_vec_dim, filter_sizes, filter_nums, word_vec_dim, dropout_rate)\n",
    "model.load_state_dict(saved_model_state) \n",
    "evaluate_trained_model(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
